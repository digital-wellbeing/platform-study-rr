[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Relationships Between Health and Logged Video Game Play Across Platforms",
    "section": "",
    "text": "This website describes the data simulation, preprocessing, and analysis code for our Stage 1 Registered Report.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Relationships Between Health and Logged Video Game Play Across Platforms</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html",
    "href": "0_generateSyntheticData.html",
    "title": "2  Generate Synthetic Data",
    "section": "",
    "text": "3 Generate Synthetic Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#intake",
    "href": "0_generateSyntheticData.html#intake",
    "title": "2  Generate Synthetic Data",
    "section": "5.1 Intake",
    "text": "5.1 Intake\n\n\nShow the code\nsynIntake &lt;- tibble(\n  pid = as.character(1:n),\n  region = rep(c(\"US\", \"UK\"), n / 2)\n) |&gt;\n  # bootstrap each column from the non-NA values in the original data\n  cbind(map_dfc(datIntake, ~ rep(sample(.[!is.na(.)], length(.), replace = TRUE), length.out = n))) |&gt;\n  # for a handful of relevant columns, we override the generate test response\n  # distributions with custom ones\n  mutate(\n    age = ifelse(region == \"US\",\n      sample(18:30, n, replace = TRUE),\n      sample(18:75, n, replace = TRUE)\n    ),\n    sex = sample(datDemo$sexProlific[!is.na(datDemo$sexProlific)],\n      n(),\n      replace = TRUE\n    ),\n    employment = sample(\n      datDemo$employmentStatusProlific[!is.na(datDemo$employmentStatusProlific) &\n        datDemo$employmentStatusProlific != \"DATA_EXPIRED\"],\n      n(),\n      replace = TRUE\n    ),\n    eduLevel = sample(datSurvey$eduLevel[!is.na(datSurvey$eduLevel)], n(), replace = TRUE),\n    ethnicity = sample(datDemo$ethnicity[!is.na(datDemo$ethnicity)], n(), replace = TRUE),\n    height = ifelse(sex == \"Male\",\n      round(rnorm(n(), 69, 3)),\n      round(rnorm(n(), 64, 3))\n    ),\n    weight = ifelse(sex == \"Male\",\n      round(rnorm(n(), 190, 35)),\n      round(rnorm(n(), 160, 35))\n    ),\n    localTimeZone = sample(datDemo$localTimeZone[!is.na(datDemo$localTimeZone)], n(), replace = TRUE)\n  ) |&gt;\n  # convert height into feet and inches\n  mutate(\n    `height#1_1_1` = height %% 12,\n    `height#1_1_2` = height %/% 12,\n    .keep = \"unused\"\n  ) |&gt;\n  # add gaming characteristics\n  mutate(\n    playsSwitch = sample(c(TRUE, FALSE), n(), prob = c(.4, .6), replace = TRUE),\n    playsXbox = sample(c(TRUE, FALSE), n(), prob = c(.5, .5), replace = TRUE),\n    playsSteam = ifelse(region == \"US\",\n      sample(c(TRUE, FALSE), n(), prob = c(.5, .5), replace = TRUE),\n      FALSE\n    ),\n    playsSteam = ifelse(!playsSwitch & !playsXbox & !playsSteam, TRUE, playsSteam), # so that all players play on at least one platform\n\n    # for people who play on a given platform, how likely are they to play on a particular day, using beta distribution\n    dailyNintendoPlayLikelihood = ifelse(playsSwitch, rbeta(n(), 2, 8), 0),\n    dailyXboxPlayLikelihood = ifelse(playsXbox, rbeta(n(), 3, 5), 0),\n    dailySteamPlayLikelihood = ifelse(playsSteam, rbeta(n(), 3, 5), 0),\n    iOSuser = sample(c(TRUE, FALSE), n(), prob = c(.7, .3), replace = TRUE),\n    androidUser = !iOSuser\n  ) |&gt;\n  copy_labels(datIntake)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#play-history",
    "href": "0_generateSyntheticData.html#play-history",
    "title": "2  Generate Synthetic Data",
    "section": "5.2 Play History",
    "text": "5.2 Play History\nNext, we want to simulate some play behavior. We assume that each player has a fixed likelihood of playing on each platform they use on a given day. If they do play, we assume they will play between 1 and 3 sessions (or in the case of Steam, that they have logged playtime in 1-5 1-hour periods of the day). We simulate those values here.\n\n\nShow the code\nsynPlayHistory &lt;- synIntake |&gt;\n  crossing(day = 1:studyDays) |&gt;\n  mutate(date = studyDates[day]) |&gt;\n  select(pid, day, date, starts_with(c(\"plays\", \"daily\"))) |&gt;\n  rowwise() |&gt;\n  mutate(\n    numSessionsNintendo = ifelse(runif(n()) &lt; dailyNintendoPlayLikelihood,\n      sample(1:3, 1, prob = c(.7, .2, .1)),\n      0\n    ),\n    numSessionsXbox = ifelse(runif(n()) &lt; dailyXboxPlayLikelihood,\n      sample(1:3, 1, prob = c(.7, .2, .1)),\n      0\n    ),\n    numHoursWithSteamPlay = ifelse(runif(n()) &lt; dailySteamPlayLikelihood,\n      sample(1:5, 1, prob = c(.4, .25, .2, .1, .05)),\n      0\n    ),\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#dropoutattrition",
    "href": "0_generateSyntheticData.html#dropoutattrition",
    "title": "2  Generate Synthetic Data",
    "section": "5.3 Dropout/attrition",
    "text": "5.3 Dropout/attrition\nHere, we simulate the properties of participant dropout. We assume that participants have a base rate of missingness at each wave (5% for diary, 10% for panel), and a separate chance each day of dropping out for the rest of the study (1% for the 84 days of the study). We simulate the missingness and dropout for each participant for each day of the study, and use this information later to remove rows from the completed panel and diary surveys.\n\n\nShow the code\ndropout &lt;- expand.grid(pid = 1:n, day = 1:studyDays) |&gt;\n  # first define the various time components of the study\n  mutate(\n    diaryWave = ifelse(day &lt;= 30, day, NA),\n    week = ceiling(day / 7),\n    panelWave = ifelse((day - 1) %% 14 == 0, (day - 1) %/% 14 + 1, NA) # return 1 on day 1, 2 on day 15, 3 on day 29, etc\n  ) |&gt;\n  arrange(as.integer(pid), day) |&gt;\n  # for simplicity, first we simulate dropout for every day of the study, then only keep the missingness on relevant days\n  mutate(\n    missingDiary = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.05, .5)),\n    missingPanel = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.1, .9)),\n    dropout = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.01, .99))\n  ) |&gt;\n  mutate(\n    missingPanel = ifelse(is.na(panelWave), NA, missingPanel),\n    missingDiary = ifelse(is.na(diaryWave), NA, missingDiary)\n  ) |&gt;\n  group_by(pid) |&gt;\n  mutate(\n    missingPanel = ifelse(cumsum(dropout) &gt; 0, TRUE, missingPanel),\n    missingDiary = ifelse(cumsum(dropout) &gt; 0, TRUE, missingDiary)\n  ) |&gt;\n  # clean up\n  ungroup() |&gt;\n  mutate(pid = as_character(pid))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#genres",
    "href": "0_generateSyntheticData.html#genres",
    "title": "2  Generate Synthetic Data",
    "section": "6.1 Genres",
    "text": "6.1 Genres\n\n\nShow the code\n# We pull from the genres present in the Nintendo metadata (which were in turn pulled from IGDB)\nuniqueGenres &lt;- datMeta |&gt;\n  filter(!is.na(genres)) |&gt; # Remove NAs\n  separate_rows(genres, sep = \",\") |&gt; # Split by comma\n  distinct(genres) |&gt; # Get unique genres\n  pull(genres) |&gt;\n  sort()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#nintendo",
    "href": "0_generateSyntheticData.html#nintendo",
    "title": "2  Generate Synthetic Data",
    "section": "6.2 Nintendo",
    "text": "6.2 Nintendo\nNext, we simulate the session-level data based on the play behavior, starting with Nintendo. For each day and session that a participant has played, we bootstrap the title, start time, and duration, from the existing data. For example, Participant 14 played Switch on Day 11, and their play was simulated to include 2 unique sessions. Games, session start times, and session durations are all drawn randomly from the real Nintendo data.\n\n\nShow the code\nsynNintendo &lt;- synPlayHistory |&gt;\n  filter(numSessionsNintendo &gt; 0) |&gt;\n  mutate(session = list(1:numSessionsNintendo)) |&gt;\n  unnest(session) |&gt;\n  mutate(\n    titleID = sample(datNintendo$titleID, n(), replace = TRUE), ,\n    sessionStart = date + seconds(runif(n(), 0, 86400)),\n    duration = sample(datNintendo$duration, n(), replace = TRUE),\n    genre = replicate(n(), assignGenres(uniqueGenres))\n  ) |&gt;\n  select(-(playsSwitch:numHoursWithSteamPlay)) |&gt;\n  mutate(platform = \"Nintendo\")\n\nglimpse(synNintendo)\n\n\nRows: 19,565\nColumns: 9\n$ pid          &lt;chr&gt; \"10\", \"10\", \"10\", \"10\", \"100\", \"100\", \"100\", \"100\", \"100\"…\n$ day          &lt;int&gt; 17, 17, 17, 31, 1, 3, 9, 15, 15, 21, 21, 27, 27, 29, 32, …\n$ date         &lt;date&gt; 2024-05-17, 2024-05-17, 2024-05-17, 2024-05-31, 2024-05-…\n$ session      &lt;int&gt; 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, …\n$ titleID      &lt;chr&gt; \"Animal Crossing New Horizons\", \"Splatoon 2\", \"Animal Cro…\n$ sessionStart &lt;dttm&gt; 2024-05-17 02:33:18, 2024-05-17 17:38:18, 2024-05-17 03:…\n$ duration     &lt;dbl&gt; 38.2, 4.3, 77.3, 4.1, 52.9, 32.7, 11.3, 101.3, 74.8, 9.7,…\n$ genre        &lt;chr&gt; \"Sport, Strategy, MOBA\", \"Puzzle, Racing\", \"MOBA\", \"Visua…\n$ platform     &lt;chr&gt; \"Nintendo\", \"Nintendo\", \"Nintendo\", \"Nintendo\", \"Nintendo…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#xbox",
    "href": "0_generateSyntheticData.html#xbox",
    "title": "2  Generate Synthetic Data",
    "section": "6.3 Xbox",
    "text": "6.3 Xbox\nWe take a similar approach for the Xbox data. However, instead of bootstrapping, we instead randomly pull from a list of an external list of Xbox games. We pull session start times from the Nintendo dataset as these are probably an equally good representation, but simulate a new set of durations as the average Xbox session is likely longer than Nintendo.\n\n\nShow the code\nxboxGames &lt;- read_csv(\"https://github.com/ItsLogic/Xbox-TitleIDs/raw/main/IDs.csv\")[1:300, ]\n\nsynXbox &lt;- synPlayHistory |&gt;\n  filter(numSessionsXbox &gt; 0) |&gt;\n  mutate(session = list(1:numSessionsXbox)) |&gt;\n  unnest(session) |&gt;\n  mutate(\n    titleID = sample(xboxGames$`Game Title`, n(), replace = TRUE),\n    sessionStart = date + seconds(runif(n(), 0, 86400)),\n    duration = round(rgamma(n(), shape = 2, rate = 1) * 60, 2),\n    genre = replicate(n(), assignGenres(uniqueGenres))\n  ) |&gt;\n  select(-(playsSwitch:numHoursWithSteamPlay)) |&gt;\n  mutate(platform = \"Xbox\")\n\nglimpse(synXbox)\n\n\nRows: 42,649\nColumns: 9\n$ pid          &lt;chr&gt; \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10…\n$ day          &lt;int&gt; 10, 10, 10, 15, 15, 16, 17, 22, 23, 24, 25, 29, 30, 30, 3…\n$ date         &lt;date&gt; 2024-05-10, 2024-05-10, 2024-05-10, 2024-05-15, 2024-05-…\n$ session      &lt;int&gt; 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, …\n$ titleID      &lt;chr&gt; \"Forza Street\", \"11-11 Memories Retold\", \"Shadow Fencer T…\n$ sessionStart &lt;dttm&gt; 2024-05-10 10:21:25, 2024-05-10 21:12:30, 2024-05-10 12:…\n$ duration     &lt;dbl&gt; 67.87, 28.36, 93.43, 193.32, 85.92, 62.88, 52.64, 79.32, …\n$ genre        &lt;chr&gt; \"Music\", \"Hack and slash/Beat 'em up\", \"Real Time Strateg…\n$ platform     &lt;chr&gt; \"Xbox\", \"Xbox\", \"Xbox\", \"Xbox\", \"Xbox\", \"Xbox\", \"Xbox\", \"…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#steam",
    "href": "0_generateSyntheticData.html#steam",
    "title": "2  Generate Synthetic Data",
    "section": "6.4 Steam",
    "text": "6.4 Steam\nNext, we move to Steam data. Here we use an existing sample of Steam data as output by Gameplay.Science (hosted on OneDrive, path specified in .Renviron).\nIn contrast to Xbox and Nintendo data, Steam data is not session-level; rather, it is a total amount of time spent playing each game during the previous hour. We simulate this by looking if the person played that day, simulating a random number of hours between 1-5 that they may have played, and then filling in an amount of time for each hour of play.\nWe also randomly assign genres to each game, as we did for Nintendo and Xbox.\n\n\nShow the code\n# note that the code structure of the steam sim is a little different, as it derives from a different process (using LLMs to generate user personas)\nsteam &lt;- read_csv(Sys.getenv(\"steamDataPath\")) |&gt;\n  filter(played == \"Yes\")\n\n# with more realistic numbers of sessions per player, the loop iteration was getting prohibitively slow\n# so we assign a random persona for each observation in synPlayHistory, then suse sampleRowByID() to only sample rows from steam\n# where the ID matches.  This is much faster, with the only downside being that different days may have identical sessions,\n# but I don't see this as a problem for the purposes of the sim (and in fact this might have been happening before anyway?)\nsynSteam &lt;- synPlayHistory |&gt;\n  mutate(persona = sample(unique(steam$ID), n(), replace = TRUE)) |&gt; # assign random persona\n  filter(numHoursWithSteamPlay &gt; 0) |&gt; # for each day where steam play occurs\n  mutate(\n    sampled_row = list(sampleRowByID(persona, steam, numHoursWithSteamPlay)) # sample a certain number of hour-sessions from that persona\n  ) |&gt;\n  unnest(cols = c(sampled_row)) |&gt;\n  # clean up\n  select(pid, day, date, hour = time, everything(), -(playsSwitch:numHoursWithSteamPlay), -ID, -played) |&gt;\n  arrange(as.integer(pid), day, date, hour) |&gt;\n  mutate(platform = \"Steam\")\n\nglimpse(synSteam)\n\n\nRows: 65,792\nColumns: 10\n$ pid      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ day      &lt;int&gt; 1, 1, 2, 3, 5, 5, 9, 10, 10, 10, 10, 11, 14, 14, 14, 15, 16, …\n$ date     &lt;date&gt; 2024-05-01, 2024-05-01, 2024-05-02, 2024-05-03, 2024-05-05, …\n$ hour     &lt;dbl&gt; 1, 16, 0, 1, 1, 2, 2, 0, 1, 2, 3, 13, 1, 2, 3, 3, 6, 12, 18, …\n$ persona  &lt;dbl&gt; 293, 293, 113, 348, 317, 317, 182, 579, 579, 579, 579, 573, 5…\n$ genre    &lt;chr&gt; \"Action,Adventure,Indie\", \"Action,Adventure,Indie\", \"Action,A…\n$ minutes  &lt;dbl&gt; 30, 30, 45, 30, 30, 25, 50, 45, 30, 50, 40, 30, 30, 50, 40, 4…\n$ AppID    &lt;dbl&gt; 231160, 2537770, 788820, 433570, 488860, 430170, 1449690, 288…\n$ Name     &lt;chr&gt; \"The Swapper\", \"Stay Still 2\", \"Lightning War\", \"Side Quest\",…\n$ platform &lt;chr&gt; \"Steam\", \"Steam\", \"Steam\", \"Steam\", \"Steam\", \"Steam\", \"Steam\"…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#ios",
    "href": "0_generateSyntheticData.html#ios",
    "title": "2  Generate Synthetic Data",
    "section": "6.5 iOS",
    "text": "6.5 iOS\niOS data consists of weekly screen time data for the below categories (pre-defined by Apple). We simulate the value of each of these categories for each participant-week using a zero-inflated gamma distribution, then add dropout as before.\n\n\nShow the code\nappCategories &lt;- c(\n  \"Entertainment\", \"Social\", \"Information & Reading\", \"Games\", \"Productivity & Finance\", \"Travel\", \"Other\",\n  \"Creativity\", \"Education\", \"Health & Fitness\", \"Shopping & Food\", \"Utilities\"\n)\n\nsyniOS &lt;- expand.grid(\n  pid = synIntake$pid[synIntake$iOSuser],\n  week = 1:(panelWaves * 2),\n  category = appCategories\n) |&gt;\n  mutate(\n    duration = sampleTime(n()),\n    date = studyDates[week * 7 - 6]\n  ) |&gt;\n  pivot_wider(names_from = category, values_from = duration, values_fill = 0) %&gt;%\n  mutate(totalScreentime = rowSums(select(., -c(pid, week, date)))) |&gt;\n  mutate(\n    missing = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.1, .9)),\n    dropout = sample(c(TRUE, FALSE), n(), replace = TRUE, prob = c(.01, .99))\n  ) |&gt;\n  group_by(pid) |&gt;\n  mutate(missing = ifelse(cumsum(dropout) &gt; 0, TRUE, missing)) |&gt;\n  ungroup() |&gt;\n  select(pid, week, date, missing, dropout, everything()) |&gt;\n  mutate(across(-c(pid, week, missing, dropout), ~ if_else(missing | dropout, NA, .))) |&gt;\n  arrange(as.integer(pid), week)\n\nglimpse(syniOS)\n\n\nRows: 16,836\nColumns: 18\n$ pid                      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, …\n$ week                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, …\n$ date                     &lt;date&gt; 2024-05-01, 2024-05-08, 2024-05-15, 2024-05-…\n$ missing                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ dropout                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ Entertainment            &lt;dbl&gt; 0.0000000, 5.7292596, 2.4548014, 1.9636654, 1…\n$ Social                   &lt;dbl&gt; 3.873012, 0.000000, 0.000000, 7.768210, 2.192…\n$ `Information & Reading`  &lt;dbl&gt; 2.0640570, 1.0929347, 0.0000000, 0.0000000, 0…\n$ Games                    &lt;dbl&gt; 2.556279, 6.762721, 2.680783, 1.584198, 1.374…\n$ `Productivity & Finance` &lt;dbl&gt; 5.0285683, 0.0000000, 0.0000000, 0.0000000, 0…\n$ Travel                   &lt;dbl&gt; 4.2951315, 4.4039391, 1.9020418, 6.9663217, 2…\n$ Other                    &lt;dbl&gt; 3.105830, 0.000000, 3.684219, 1.378007, 5.816…\n$ Creativity               &lt;dbl&gt; 2.6572140, 3.0750600, 4.2126908, 1.5049544, 0…\n$ Education                &lt;dbl&gt; 3.5425213, 1.0298568, 4.2429778, 1.2054966, 0…\n$ `Health & Fitness`       &lt;dbl&gt; 4.7835503, 2.4794736, 0.0000000, 0.0000000, 1…\n$ `Shopping & Food`        &lt;dbl&gt; 0.0000000, 0.0000000, 5.0112091, 0.0000000, 2…\n$ Utilities                &lt;dbl&gt; 2.8956759, 0.0000000, 0.0000000, 1.1620841, 0…\n$ totalScreentime          &lt;dbl&gt; 34.80184, 24.57324, 24.18872, 23.53294, 17.75…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#android",
    "href": "0_generateSyntheticData.html#android",
    "title": "2  Generate Synthetic Data",
    "section": "6.6 Android",
    "text": "6.6 Android\nLast, we simulate android data, bootstrapping from a sample ActivityWatch output (hosted on OneDrive, path specified in .Renviron).\n\n\nShow the code\n# Extract and normalize the 'events' data - app sessions - which are found in the 'aw-watcher-android-test' bucket\nandroidEvents &lt;- datAndroid$buckets$`aw-watcher-android-test`$events |&gt;\n  as_tibble() |&gt;\n  flatten()\n\nsynAndroid &lt;- expand.grid(\n  pid = synIntake$pid[synIntake$androidUser],\n  day = 1:studyDays\n) |&gt;\n  mutate(\n    numDailyAppSessions = round(rgamma(n(), 10, .5)),\n    date = studyDates[day]\n  ) |&gt; # simulate a random number of app sessions to have taken place that day\n  rowwise() |&gt;\n  mutate(session = list(1:numDailyAppSessions)) |&gt;\n  unnest(session) |&gt;\n  mutate(\n    app = sample(androidEvents$data.app, n(), replace = TRUE), ,\n    sessionStart = as_hms(sample(ymd_hms(androidEvents$timestamp), n(), replace = TRUE)),\n    duration = sample(androidEvents$duration, n(), replace = TRUE),\n  ) |&gt;\n  # for simplicity at the simulation stage, we simply random assign categories to each app, matching the iOS categories\n  # in the full paper, we will map apps to categories using the google play store API\n  group_by(app) |&gt;\n  mutate(category = sample(appCategories, 1, replace = TRUE)) |&gt;\n  ungroup() |&gt;\n  arrange(as.integer(pid), day, sessionStart)\n\nglimpse(synAndroid)\n\n\nRows: 1,001,307\nColumns: 9\n$ pid                 &lt;fct&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ day                 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ numDailyAppSessions &lt;dbl&gt; 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26…\n$ date                &lt;date&gt; 2024-05-01, 2024-05-01, 2024-05-01, 2024-05-01, 2…\n$ session             &lt;int&gt; 7, 6, 15, 26, 17, 1, 23, 3, 21, 13, 20, 8, 9, 14, …\n$ app                 &lt;chr&gt; \"Kiwi Browser\", \"Samsung Internet\", \"Spotify\", \"Tr…\n$ sessionStart        &lt;time&gt; 06:25:37.173, 07:02:26.188, 08:01:16.571, 08:06:0…\n$ duration            &lt;dbl&gt; 0.085, 1.168, 6.528, 22.051, 27.117, 1.377, 4.638,…\n$ category            &lt;chr&gt; \"Utilities\", \"Entertainment\", \"Travel\", \"Social\", …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#panel",
    "href": "0_generateSyntheticData.html#panel",
    "title": "2  Generate Synthetic Data",
    "section": "7.1 Panel",
    "text": "7.1 Panel\n\n\nShow the code\nsynPanel &lt;- expand.grid(pid = as.character(1:n), wave = 1:panelWaves) |&gt;\n  # add random completion time between 12pm and 12pm the following day\n  mutate(\n    date = studyDates[wave * 14 - 13],\n    surveyCompletionTime = date + seconds(runif(n(), 43200, 129600))\n  ) |&gt;\n  # bootstrap each column from the non-NA values in the original data\n  cbind(map_dfc(datPanel, ~ rep(sample(.[!is.na(.)], length(.), replace = TRUE), length.out = n * panelWaves))) |&gt;\n  # certain measures are only administered at certain waves, set these to 0 for\n  # the other waves\n  mutate(across(starts_with(c(\"BFI\", \"trojan\", \"mctq\")), ~ ifelse(wave != 1, NA, .))) |&gt;\n  mutate(across(starts_with(\"gdt\"), ~ ifelse(wave %in% c(2:5), NA, .))) |&gt;\n  mutate(across(starts_with(c(\"psqi\", \"eps\")), ~ ifelse(wave %in% c(1, 3, 5), NA, .))) |&gt;\n  # join with the relevant rows/cols of the missingness df\n  left_join(dropout |&gt; filter(!is.na(panelWave)) |&gt; select(pid, panelWave, missingPanel),\n    by = c(\"pid\", \"wave\" = \"panelWave\")\n  ) |&gt;\n  select(pid, wave, date, missingPanel, everything()) |&gt;\n  # remove data for all missing diary waves\n  mutate(across(-c(pid, wave, missingPanel), ~ if_else(missingPanel, NA, .))) |&gt;\n  arrange(as.numeric(pid), wave) |&gt;\n  copy_labels(datPanel)\n\n\n\n\nClick to view the structure of the panel data\n\n\nglimpse(synPanel)\n\nRows: 12,000\nColumns: 165\n$ pid                     &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"…\n$ wave                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4…\n$ date                    &lt;date&gt; 2024-05-01, 2024-05-15, 2024-05-29, 2024-06-1…\n$ missingPanel            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE,…\n$ surveyCompletionTime    &lt;dttm&gt; 2024-05-02 04:46:30, 2024-05-16 07:26:35, 202…\n$ problematicPlay         &lt;chr&gt; \"Phasellus. Est diam per erat integer wisi nul…\n$ positives               &lt;chr&gt; \"Ultrices per ab massa ac euismod blandit sem …\n$ gdt_1                   &lt;chr&gt; \"Sometimes\", NA, NA, NA, NA, NA, \"Often\", NA, …\n$ gdt_2                   &lt;chr&gt; \"Sometimes\", NA, NA, NA, NA, NA, \"Sometimes\", …\n$ gdt_3                   &lt;chr&gt; \"Never\", NA, NA, NA, NA, NA, \"Often\", NA, NA, …\n$ gdt_4                   &lt;chr&gt; \"Often\", NA, NA, NA, NA, NA, \"Often\", NA, NA, …\n$ mctq_1                  &lt;int&gt; 2, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, …\n$ mctq_2_1                &lt;int&gt; 6, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, …\n$ mctq_3_1                &lt;chr&gt; \"15:59\", NA, NA, NA, NA, NA, \"10:03\", NA, NA, …\n$ mctq_3_2                &lt;chr&gt; \"Enim felis curabitur? Molestie eu sagittis ne…\n$ mctq_3_3                &lt;chr&gt; \"00:27\", NA, NA, NA, NA, NA, \"17:41\", NA, NA, …\n$ mctq_3_4                &lt;dbl&gt; 39.20008, NA, NA, NA, NA, NA, 44.62950, NA, NA…\n$ mctq_3_5                &lt;chr&gt; \"00:13\", NA, NA, NA, NA, NA, \"22:14\", NA, NA, …\n$ mctq_3_6                &lt;dbl&gt; 5.796318, NA, NA, NA, NA, NA, 47.058190, NA, N…\n$ mctq_4_1                &lt;chr&gt; \"No\", NA, NA, NA, NA, NA, \"Yes\", NA, NA, NA, N…\n$ mctq_5_1                &lt;chr&gt; \"No\", NA, NA, NA, NA, NA, \"No\", NA, NA, NA, NA…\n$ mctq_6_1                &lt;chr&gt; \"20:48\", NA, NA, NA, NA, NA, \"04:16\", NA, NA, …\n$ mctq_6_2                &lt;chr&gt; \"Sed eleifend proin rutrum. Orci culpa at luct…\n$ mctq_6_3                &lt;chr&gt; \"14:11\", NA, NA, NA, NA, NA, \"11:33\", NA, NA, …\n$ mctq_6_4                &lt;dbl&gt; 13.251418, NA, NA, NA, NA, NA, 56.047642, NA, …\n$ mctq_6_5                &lt;chr&gt; \"12:16\", NA, NA, NA, NA, NA, \"08:19\", NA, NA, …\n$ mctq_6_6                &lt;dbl&gt; 6.658175, NA, NA, NA, NA, NA, 37.259872, NA, N…\n$ mctq_7_1                &lt;chr&gt; \"Yes\", NA, NA, NA, NA, NA, \"No\", NA, NA, NA, N…\n$ mctq_7_2                &lt;chr&gt; \"Yes\", NA, NA, NA, NA, NA, \"Yes\", NA, NA, NA, …\n$ mctq_8_1                &lt;chr&gt; \"Hobbies\", NA, NA, NA, NA, NA, \"Child(ren)/pet…\n$ mctq_9                  &lt;chr&gt; \"Auctor neque per? Quam nonummy leo ligula sed…\n$ `psqi_1#1_1_1`          &lt;dbl&gt; NA, 30320142, NA, 109754050, NA, NA, NA, 14886…\n$ `psqi_1#1_1_2`          &lt;dbl&gt; NA, 543316285, NA, 833081673, NA, NA, NA, 9163…\n$ `psqi_1#2_1`            &lt;chr&gt; NA, \"AM\", NA, \"AM\", NA, NA, NA, \"PM\", NA, \"AM\"…\n$ psqi_2                  &lt;dbl&gt; NA, 546670535, NA, 1772158820, NA, NA, NA, 189…\n$ `psqi_3#1_1_1`          &lt;dbl&gt; NA, 1784034938, NA, 1489034961, NA, NA, NA, 20…\n$ `psqi_3#1_1_2`          &lt;dbl&gt; NA, 1786659833, NA, 924252618, NA, NA, NA, 166…\n$ `psqi_3#2_1`            &lt;chr&gt; NA, \"PM\", NA, \"AM\", NA, NA, NA, \"AM\", NA, \"PM\"…\n$ `psqi_4#1_1_1`          &lt;dbl&gt; NA, 1359004739, NA, 733074959, NA, NA, NA, 237…\n$ `psqi_4#1_1_2`          &lt;dbl&gt; NA, 1502688050, NA, 1575807686, NA, NA, NA, 18…\n$ psqi_5_1                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Less than onc…\n$ psqi_5_2                &lt;chr&gt; NA, \"Less than once a week\", NA, \"Three or mor…\n$ psqi_5_3                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Once or twice…\n$ psqi_5_4                &lt;chr&gt; NA, \"Three or more times a week\", NA, \"Once or…\n$ psqi_5_5                &lt;chr&gt; NA, \"Less than once a week\", NA, \"Three or mor…\n$ psqi_5_6                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Not during th…\n$ psqi_5_7                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Three or more…\n$ psqi_5_8                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Once or twice…\n$ psqi_5_9                &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Not during th…\n$ psqi_5_c                &lt;chr&gt; NA, \"Etiam nec nullam blandit pede duis porta …\n$ psqi_5_cr_1             &lt;chr&gt; NA, \"Not during the past month\", NA, \"Less tha…\n$ psqi_6                  &lt;int&gt; NA, 3, NA, 2, NA, NA, NA, 2, NA, 3, NA, 4, NA,…\n$ psqi_7                  &lt;int&gt; NA, 3, NA, 1, NA, NA, NA, 1, NA, 3, NA, 3, NA,…\n$ psqi_8                  &lt;int&gt; NA, 3, NA, 2, NA, NA, NA, 4, NA, 2, NA, 1, NA,…\n$ psqi_9                  &lt;int&gt; NA, 4, NA, 2, NA, NA, NA, 3, NA, 3, NA, 2, NA,…\n$ psqi_10                 &lt;int&gt; NA, 2, NA, 4, NA, NA, NA, 4, NA, 1, NA, 1, NA,…\n$ psqi_10_c_1             &lt;chr&gt; NA, \"Three or more times a week\", NA, \"Once or…\n$ psqi_10_c_2             &lt;chr&gt; NA, \"Less than once a week\", NA, \"Once or twic…\n$ psqi_10_c_3             &lt;chr&gt; NA, \"Not during the past month\", NA, \"Less tha…\n$ psqi_10_c_4             &lt;chr&gt; NA, \"Less than once a week\", NA, \"Once or twic…\n$ psqi_10_c_5             &lt;chr&gt; NA, \"Once or twice a week\", NA, \"Less than onc…\n$ psqi_10_c_5_TEXT        &lt;chr&gt; NA, \"Viverra? Praesent elit pharetra. Orci? Du…\n$ eps_1_1                 &lt;chr&gt; NA, \"No chance of dozing\", NA, \"High chance of…\n$ eps_1_2                 &lt;chr&gt; NA, \"No chance of dozing\", NA, \"No chance of d…\n$ eps_1_3                 &lt;chr&gt; NA, \"No chance of dozing\", NA, \"High chance of…\n$ eps_1_4                 &lt;chr&gt; NA, \"No chance of dozing\", NA, \"High chance of…\n$ eps_1_5                 &lt;chr&gt; NA, \"Slight chance of dozing\", NA, \"No chance …\n$ eps_1_6                 &lt;chr&gt; NA, \"High chance of dozing\", NA, \"Moderate cha…\n$ eps_1_7                 &lt;chr&gt; NA, \"High chance of dozing\", NA, \"No chance of…\n$ eps_1_8                 &lt;chr&gt; NA, \"No chance of dozing\", NA, \"High chance of…\n$ `BFI-2-XS_1`            &lt;chr&gt; \"Neutral; no opinion\", NA, NA, NA, NA, NA, \"Di…\n$ `BFI-2-XS_2`            &lt;chr&gt; \"Disagree a little\", NA, NA, NA, NA, NA, \"Agre…\n$ `BFI-2-XS_3`            &lt;chr&gt; \"Agree a little\", NA, NA, NA, NA, NA, \"Neutral…\n$ `BFI-2-XS_4`            &lt;chr&gt; \"Neutral; no opinion\", NA, NA, NA, NA, NA, \"Ag…\n$ `BFI-2-XS_5`            &lt;chr&gt; \"Agree a little\", NA, NA, NA, NA, NA, \"Agree a…\n$ `BFI-2-XS_6`            &lt;chr&gt; \"Neutral; no opinion\", NA, NA, NA, NA, NA, \"Ne…\n$ `BFI-2-XS_7`            &lt;chr&gt; \"Disagree strongly\", NA, NA, NA, NA, NA, \"Disa…\n$ `BFI-2-XS_8`            &lt;chr&gt; \"Disagree strongly\", NA, NA, NA, NA, NA, \"Disa…\n$ `BFI-2-XS_9`            &lt;chr&gt; \"Neutral; no opinion\", NA, NA, NA, NA, NA, \"Ne…\n$ `BFI-2-XS_10`           &lt;chr&gt; \"Disagree a little\", NA, NA, NA, NA, NA, \"Neut…\n$ `BFI-2-XS_11`           &lt;chr&gt; \"Neutral; no opinion\", NA, NA, NA, NA, NA, \"Ag…\n$ `BFI-2-XS_12`           &lt;chr&gt; \"Disagree a little\", NA, NA, NA, NA, NA, \"Disa…\n$ `BFI-2-XS_13`           &lt;chr&gt; \"Agree strongly\", NA, NA, NA, NA, NA, \"Neutral…\n$ `BFI-2-XS_14`           &lt;chr&gt; \"Disagree strongly\", NA, NA, NA, NA, NA, \"Agre…\n$ `BFI-2-XS_15`           &lt;chr&gt; \"Disagree a little\", NA, NA, NA, NA, NA, \"Disa…\n$ trojan_1                &lt;chr&gt; \"3\", NA, NA, NA, NA, NA, \"2\", NA, NA, NA, NA, …\n$ trojan_2                &lt;chr&gt; \"3\", NA, NA, NA, NA, NA, \"4\", NA, NA, NA, NA, …\n$ trojan_3                &lt;chr&gt; \"4\", NA, NA, NA, NA, NA, \"1 - Strongly disagre…\n$ trojan_4                &lt;chr&gt; \"2\", NA, NA, NA, NA, NA, \"3\", NA, NA, NA, NA, …\n$ trojan_5                &lt;chr&gt; \"1 - Strongly disagree\", NA, NA, NA, NA, NA, \"…\n$ trojan_6                &lt;chr&gt; \"3\", NA, NA, NA, NA, NA, \"2\", NA, NA, NA, NA, …\n$ trojan_7                &lt;chr&gt; \"4\", NA, NA, NA, NA, NA, \"2\", NA, NA, NA, NA, …\n$ trojan_8                &lt;chr&gt; \"4\", NA, NA, NA, NA, NA, \"4\", NA, NA, NA, NA, …\n$ trojan_9                &lt;chr&gt; \"2\", NA, NA, NA, NA, NA, \"4\", NA, NA, NA, NA, …\n$ trojan_10               &lt;chr&gt; \"1 - Strongly disagree\", NA, NA, NA, NA, NA, \"…\n$ trojan_11               &lt;chr&gt; \"2\", NA, NA, NA, NA, NA, \"3\", NA, NA, NA, NA, …\n$ trojan_12               &lt;chr&gt; \"2\", NA, NA, NA, NA, NA, \"1 - Strongly disagre…\n$ trojan_13               &lt;chr&gt; \"3\", NA, NA, NA, NA, NA, \"3\", NA, NA, NA, NA, …\n$ trojan_14               &lt;chr&gt; \"4\", NA, NA, NA, NA, NA, \"4\", NA, NA, NA, NA, …\n$ trojan_15               &lt;chr&gt; \"1 - Strongly disagree\", NA, NA, NA, NA, NA, \"…\n$ `selfreportPlay #1_1_1` &lt;dbl&gt; 451864991, 1854152882, 1715559403, 517546130, …\n$ `selfreportPlay #1_1_2` &lt;dbl&gt; 1573216261, 1923083539, 758663418, 1385740675,…\n$ `selfreportPlay #1_2_1` &lt;dbl&gt; 440687077, 868777931, 1138501115, 442426895, N…\n$ `selfreportPlay #1_2_2` &lt;dbl&gt; 328526617, 1627195134, 2053221620, 135050009, …\n$ `selfreportPlay #1_3_1` &lt;dbl&gt; 83312732, 865433634, 1638742542, 1343099576, N…\n$ `selfreportPlay #1_3_2` &lt;dbl&gt; 1951798264, 1769641675, 359766896, 1627439538,…\n$ recentSessions_1_1      &lt;chr&gt; \"Malesuada, convallis mattis purus accusamus m…\n$ recentSessions_1_2      &lt;chr&gt; \"Venenatis convallis. Sollicitudin blandit et …\n$ recentSessions_1_3      &lt;chr&gt; \"Potenti sagittis laoreet tellus magna. Dolor …\n$ recentSessions_1_4      &lt;chr&gt; \"Blandit est tempus a non donec platea felis. …\n$ recentSessions_2_1      &lt;chr&gt; \"Luctus maecenas malesuada elit! Sem commodo, …\n$ recentSessions_2_2      &lt;chr&gt; \"Tortor? Vitae cursus elit quam ut dolor at. M…\n$ recentSessions_2_3      &lt;chr&gt; \"Augue! Quam a id viverra interdum ipsum ultri…\n$ recentSessions_2_4      &lt;chr&gt; \"Diam! Tempus tortor tempor porta. Cursus wisi…\n$ recentSessions_3_1      &lt;chr&gt; \"Venenatis faucibus diam velit egestas malesua…\n$ recentSessions_3_2      &lt;chr&gt; \"Massa nibh per nulla orci ipsum. Laoreet dui …\n$ recentSessions_3_3      &lt;chr&gt; \"Platea nonummy sit dolorem. Pretium viverra! …\n$ recentSessions_3_4      &lt;chr&gt; \"Arcu interdum nullam per curabitur! Tempus ut…\n$ csas                    &lt;dbl&gt; 3, 4, 1, 9, NA, NA, 0, 0, 8, 0, NA, 9, 10, 3, …\n$ affectiveValence_1      &lt;dbl&gt; 44, 23, 30, 44, NA, NA, 55, 27, 10, 6, NA, 76,…\n$ wemwbs_1                &lt;chr&gt; \"2 - Rarely\", \"1 - None of the time\", \"3 - Som…\n$ wemwbs_2                &lt;chr&gt; \"5 - All of the time\", \"4 - Often\", \"2 - Rarel…\n$ wemwbs_3                &lt;chr&gt; \"5 - All of the time\", \"1 - None of the time\",…\n$ wemwbs_4                &lt;chr&gt; \"2 - Rarely\", \"2 - Rarely\", \"4 - Often\", \"2 - …\n$ wemwbs_5                &lt;chr&gt; \"5 - All of the time\", \"3 - Some of the time\",…\n$ wemwbs_6                &lt;chr&gt; \"2 - Rarely\", \"3 - Some of the time\", \"5 - All…\n$ wemwbs_7                &lt;chr&gt; \"2 - Rarely\", \"3 - Some of the time\", \"2 - Rar…\n$ wemwbs_attCheck_1       &lt;chr&gt; \"5 - All of the time\", \"1 - None of the time\",…\n$ wemwbs_attCheck_2       &lt;chr&gt; \"2 - Rarely\", \"4 - Often\", \"1 - None of the ti…\n$ wemwbs_attCheck_3       &lt;chr&gt; \"2 - Rarely\", \"1 - None of the time\", \"2 - Rar…\n$ wemwbs_attCheck_4       &lt;chr&gt; \"5 - All of the time\", \"2 - Rarely\", \"1 - None…\n$ wemwbs_attCheck_5       &lt;chr&gt; \"4 - Often\", \"4 - Often\", \"1 - None of the tim…\n$ wemwbs_attCheck_6       &lt;chr&gt; \"1 - None of the time\", \"3 - Some of the time\"…\n$ wemwbs_attCheck_7       &lt;chr&gt; \"5 - All of the time\", \"3 - Some of the time\",…\n$ promis_1                &lt;chr&gt; \"Rarely\", \"Never\", \"Sometimes\", \"Rarely\", NA, …\n$ promis_2                &lt;chr&gt; \"Always\", \"Rarely\", \"Rarely\", \"Often\", NA, NA,…\n$ promis_3                &lt;chr&gt; \"Never\", \"Often\", \"Often\", \"Always\", NA, NA, \"…\n$ promis_4                &lt;chr&gt; \"Never\", \"Always\", \"Often\", \"Never\", NA, NA, \"…\n$ promis_5                &lt;chr&gt; \"Sometimes\", \"Never\", \"Never\", \"Often\", NA, NA…\n$ promis_6                &lt;chr&gt; \"Sometimes\", \"Rarely\", \"Rarely\", \"Never\", NA, …\n$ promis_7                &lt;chr&gt; \"Rarely\", \"Often\", \"Sometimes\", \"Rarely\", NA, …\n$ promis_8                &lt;chr&gt; \"Never\", \"Sometimes\", \"Often\", \"Sometimes\", NA…\n$ bangs_1                 &lt;chr&gt; \"2\", \"4Neither Agree nor Disagree\", \"2\", \"3\", …\n$ bangs_2                 &lt;chr&gt; \"1 \\nStrongly Disagree\", \"3\", \"3\", \"1 \\nStrong…\n$ bangs_3                 &lt;chr&gt; \"2\", \"5\", \"7 Strongly agree\", \"2\", NA, NA, \"5\"…\n$ bangs_4                 &lt;chr&gt; \"4Neither Agree nor Disagree\", \"5\", \"2\", \"3\", …\n$ bangs_5                 &lt;chr&gt; \"7 Strongly agree\", \"2\", \"1 \\nStrongly Disagre…\n$ bangs_6                 &lt;chr&gt; \"4Neither Agree nor Disagree\", \"2\", \"3\", \"7 St…\n$ bangs_7                 &lt;chr&gt; \"7 Strongly agree\", \"1 \\nStrongly Disagree\", \"…\n$ bangs_8                 &lt;chr&gt; \"7 Strongly agree\", \"3\", \"7 Strongly agree\", \"…\n$ bangs_9                 &lt;chr&gt; \"6\", \"5\", \"5\", \"2\", NA, NA, \"1 \\nStrongly Disa…\n$ bangs_10                &lt;chr&gt; \"5\", \"1 \\nStrongly Disagree\", \"7 Strongly agre…\n$ bangs_11                &lt;chr&gt; \"2\", \"7 Strongly agree\", \"3\", \"2\", NA, NA, \"4N…\n$ bangs_12                &lt;chr&gt; \"2\", \"5\", \"7 Strongly agree\", \"1 \\nStrongly Di…\n$ bangs_13                &lt;chr&gt; \"2\", \"2\", \"3\", \"6\", NA, NA, \"1 \\nStrongly Disa…\n$ bangs_14                &lt;chr&gt; \"7 Strongly agree\", \"3\", \"3\", \"6\", NA, NA, \"3\"…\n$ bangs_15                &lt;chr&gt; \"1 \\nStrongly Disagree\", \"4Neither Agree nor D…\n$ bangs_16                &lt;chr&gt; \"4Neither Agree nor Disagree\", \"2\", \"7 Strongl…\n$ bangs_17                &lt;chr&gt; \"6\", \"1 \\nStrongly Disagree\", \"6\", \"5\", NA, NA…\n$ bangs_18                &lt;chr&gt; \"3\", \"2\", \"5\", \"1 \\nStrongly Disagree\", NA, NA…\n$ displacement_1          &lt;chr&gt; \"Greatly supported\", \"Slightly interfered\", \"G…\n$ displacement_2          &lt;chr&gt; \"Greatly interfered\", \"Slightly supported\", \"M…\n$ displacement_3          &lt;chr&gt; \"Slightly supported\", \"No impact\", \"Moderately…\n$ displacement_4          &lt;chr&gt; \"Slightly supported\", \"No impact\", \"Greatly su…\n$ displacement_5          &lt;chr&gt; \"Greatly supported\", \"No impact\", \"Moderately …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "0_generateSyntheticData.html#diary",
    "href": "0_generateSyntheticData.html#diary",
    "title": "2  Generate Synthetic Data",
    "section": "7.2 Diary",
    "text": "7.2 Diary\n\n\nShow the code\nsynDiary &lt;- expand.grid(pid = synIntake$pid[synIntake$region == \"US\"], day = 1:diaryWaves) |&gt;\n  mutate(\n    date = studyDates[day],\n    surveyCompletionTime = date + seconds(runif(n(), 64800, 93600)) # add time so that the survey is randomly completed between 6pm and 2am\n  ) |&gt;\n  # bootstrap each column from the non-NA values in the original data\n  cbind(map_dfc(datDiary, ~ rep(sample(.[!is.na(.)], length(.), replace = TRUE),\n    length.out = length(synIntake$pid[synIntake$region == \"US\"]) * diaryWaves\n  ))) |&gt;\n  # join with missingness df\n  left_join(dropout |&gt; filter(!is.na(diaryWave)) |&gt; select(pid, diaryWave, missingDiary),\n    by = c(\"pid\", \"day\" = \"diaryWave\")\n  ) |&gt;\n  # remove data for all missing diary waves, and removing gaming data when the\n  # participant didn't play in the last 24 hours\n  mutate(\n    across(-c(pid, day, date, missingDiary), ~ if_else(missingDiary, NA, .)),\n    across(starts_with(c(\"socialGaming\", \"bangs\", \"mostRecentGame\", \"displaced\")), ~ ifelse(played24hr == \"No\", NA, .)) # remove gaming data\n  ) |&gt;\n  # clean up\n  select(pid, day, date, missingDiary, everything()) |&gt;\n  arrange(as.integer(pid), day) |&gt;\n  copy_labels(datDiary)\n\n\n\n\nClick to view the structure of the diary data\n\n\nglimpse(synDiary)\n\nRows: 21,000\nColumns: 70\n$ pid                  &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ day                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date                 &lt;date&gt; 2024-05-01, 2024-05-02, 2024-05-03, 2024-05-04, …\n$ missingDiary         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ surveyCompletionTime &lt;dttm&gt; 2024-05-01 19:17:25, 2024-05-02 19:12:14, 2024-0…\n$ played24hr           &lt;ord&gt; Yes, Yes, No, Yes, Yes, Yes, No, No, Yes, No, No,…\n$ socialGaming_4       &lt;chr&gt; \"Single-player games only\", \"Single-player games …\n$ socialGaming_5       &lt;chr&gt; \"Multiplayer with real-world friends\", \"Multiplay…\n$ socialGaming_6       &lt;chr&gt; \"Multiplayer with online-only friends\", \"Multipla…\n$ socialGaming_7       &lt;chr&gt; \"Multiplayer with strangers\", \"Multiplayer with s…\n$ mostRecentGame       &lt;chr&gt; \"Rhoncus lacus egestas imperdiet augue mo\", \"Prae…\n$ bangs_1              &lt;chr&gt; \"2\", \"5\", NA, \"4Neither Agree nor Disagree\", \"5\",…\n$ bangs_2              &lt;chr&gt; \"2\", \"3\", NA, \"1 \\nStrongly Disagree\", \"4Neither …\n$ bangs_3              &lt;chr&gt; \"5\", \"3\", NA, \"4Neither Agree nor Disagree\", \"6\",…\n$ bangs_4              &lt;chr&gt; \"4Neither Agree nor Disagree\", \"1 \\nStrongly Disa…\n$ bangs_5              &lt;chr&gt; \"5\", \"6\", NA, \"7 Strongly agree\", \"5\", \"6\", NA, N…\n$ bangs_6              &lt;chr&gt; \"2\", \"4Neither Agree nor Disagree\", NA, \"5\", \"4Ne…\n$ displacedActivity    &lt;chr&gt; \"Nonummy placerat? Phasellus placerat. Tincidunt …\n$ lifeSat_1            &lt;dbl&gt; 32, 98, 89, 30, 59, 51, 51, 75, 57, 85, 33, 74, 7…\n$ affectiveValence_1   &lt;dbl&gt; 52, 95, 43, 81, 65, 92, 52, 38, 46, 57, 60, 81, 4…\n$ bpnsfs_1             &lt;chr&gt; \"2 - strongly disagree\", \"4 - neither disagree no…\n$ bpnsfs_2             &lt;chr&gt; \"4 - neither disagree nor agree\", \"5 - agree\", \"3…\n$ bpnsfs_3             &lt;chr&gt; \"5 - agree\", \"3 - disagree\", \"5 - agree\", \"1 - ve…\n$ bpnsfs_4             &lt;chr&gt; \"5 - agree\", \"3 - disagree\", \"4 - neither disagre…\n$ bpnsfs_5             &lt;chr&gt; \"3 - disagree\", \"4 - neither disagree nor agree\",…\n$ bpnsfs_6             &lt;chr&gt; \"5 - agree\", \"1 - very strongly disagree\", \"1 - v…\n$ sd_0                 &lt;ord&gt; Vacation day, Other (please specify):, Weekend, H…\n$ sd_0_8_TEXT          &lt;chr&gt; \"Felis? Posuere magna ipsum volutpat, gravida ab …\n$ `sd_1#1_1_1`         &lt;dbl&gt; 1603849685, 894379514, 735841771, 1602488313, 128…\n$ `sd_1#1_1_2`         &lt;dbl&gt; 1567469283, 1691810101, 930545705, 862698895, 791…\n$ `sd_1#2_1`           &lt;chr&gt; \"AM\", \"PM\", \"PM\", \"AM\", \"PM\", \"PM\", \"AM\", \"AM\", \"…\n$ `sd_2#1_1_1`         &lt;dbl&gt; 485500086, 104054518, 1169408008, 140056431, 1836…\n$ `sd_2#1_1_2`         &lt;dbl&gt; 1204348752, 1861270655, 545564080, 238349578, 981…\n$ `sd_2#2_1`           &lt;chr&gt; \"AM\", \"PM\", \"PM\", \"PM\", \"PM\", \"AM\", \"PM\", \"AM\", \"…\n$ sd_3                 &lt;dbl&gt; 2132146993, 1606313892, 1789753926, 1804958614, 1…\n$ `sd_4#1_1_1`         &lt;dbl&gt; 466056748, 307429782, 1257880481, 286159513, 1318…\n$ `sd_4#1_1_2`         &lt;dbl&gt; 851300260, 1380056229, 719718577, 870500403, 1275…\n$ `sd_4#2_1`           &lt;chr&gt; \"AM\", \"PM\", \"AM\", \"AM\", \"PM\", \"PM\", \"PM\", \"AM\", \"…\n$ `sd_5#1_1_1`         &lt;dbl&gt; 341739502, 242494822, 1001144473, 1130461394, 208…\n$ `sd_5#1_1_2`         &lt;dbl&gt; 1255753273, 1468794244, 824672027, 2002728982, 51…\n$ `sd_5#2_1`           &lt;chr&gt; \"AM\", \"AM\", \"PM\", \"PM\", \"AM\", \"PM\", \"PM\", \"PM\", \"…\n$ sd_6                 &lt;ord&gt; Good, Very good, Fair, Very poor, Very poor, Very…\n$ timeUse_5            &lt;dbl&gt; 6, 10, 7, 10, 0, 13, 14, 4, 1, 13, 2, 13, 5, 0, 8…\n$ timeUse_6            &lt;dbl&gt; 14, 1, 14, 1, 10, 7, 7, 6, 9, 12, 3, 1, 4, 10, 4,…\n$ timeUse_1            &lt;dbl&gt; 7, 7, 4, 1, 2, 0, 6, 4, 0, 1, 3, 3, 1, 0, 10, 1, …\n$ timeUse_2            &lt;dbl&gt; 0, 2, 0, 7, 6, 3, 1, 7, 0, 2, 11, 2, 1, 0, 2, 1, …\n$ timeUse_3            &lt;dbl&gt; 1.0, 0.0, 0.0, 0.0, 4.1, 0.0, 8.0, 1.0, 0.0, 0.0,…\n$ timeUse_4            &lt;dbl&gt; 2, 0, 4, 2, 0, 0, 2, 1, 0, 5, 2, 0, 1, 0, 0, 0, 2…\n$ timeUse_7            &lt;dbl&gt; 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2…\n$ timeUse_8            &lt;dbl&gt; 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ timeUse_9            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ timeUse_10           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ timeUse_11           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ timeUse_12           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ timeUse_13           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ timeUse_14           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ stress_argument      &lt;ord&gt; No, No, Yes, No, No, Yes, No, No, No, Yes, Yes, N…\n$ howStress_arg        &lt;chr&gt; \"Not very\", \"Not at all\", \"Very\", \"Somewhat\", \"Ve…\n$ stress_letpass       &lt;ord&gt; Yes, Yes, No, No, Yes, Yes, Yes, No, Yes, Yes, Ye…\n$ howStress_letPass    &lt;ord&gt; Somewhat, Very, Not very, Not very, Not very, Som…\n$ stress_school        &lt;ord&gt; No, Yes, No, No, Yes, Yes, No, Yes, Yes, No, No, …\n$ howStress_school     &lt;ord&gt; Not very, Not very, Very, Somewhat, Somewhat, Som…\n$ stress_home          &lt;ord&gt; No, Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Ye…\n$ howStress_home       &lt;ord&gt; Not very, Very, Not very, Very, Not at all, Very,…\n$ stress_discrim       &lt;ord&gt; No, No, No, No, No, Yes, No, No, No, No, Yes, No,…\n$ howStress_discrim    &lt;ord&gt; Not at all, Not at all, Not at all, Not at all, S…\n$ stress_relative      &lt;ord&gt; No, Yes, Yes, No, No, No, Yes, Yes, Yes, No, No, …\n$ howStress_relative   &lt;ord&gt; Not very, Very, Not at all, Somewhat, Very, Very,…\n$ stress_other         &lt;ord&gt; No, No, No, No, No, Yes, No, Yes, No, Yes, Yes, Y…\n$ howStress_other      &lt;ord&gt; Very, Not at all, Not at all, Very, Very, Not ver…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generate Synthetic Data</span>"
    ]
  },
  {
    "objectID": "1_preprocess.html",
    "href": "1_preprocess.html",
    "title": "3  Preprocess Data",
    "section": "",
    "text": "Note\n\n\n\nThis is very much WIP - I’m adding in the preprocessing of measures/variables as and when we need them for particular models.\nAll will be cleaned and tidied in due course!\n\n\n\n4 Preamble\n\n\nShow the code\nknitr::opts_chunk$set(\n  echo = knitr::is_html_output(),\n  warning = FALSE,\n  message = FALSE,\n  output = TRUE\n)\n\nset.seed(8675309)\n\n\n\n\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\nlibrary(pacman)\n\np_load(tidyverse)\n\n\n\n\n5 Load data\n\n\nShow the code\nsynDiary &lt;- read_csv(\"data-synthetic/synDiary.csv\")\nsynPanel &lt;- read_csv(\"data-synthetic/synPanel.csv\")\nnin &lt;- read_csv(\"data-synthetic/synNintendo.csv\") |&gt;\n  mutate(sessionEnd = sessionStart + minutes(round(duration)), .after = sessionStart)\nxbox &lt;- read_csv(\"data-synthetic/synXbox.csv\") |&gt;\n  mutate(sessionEnd = sessionStart + minutes(round(duration)), .after = sessionStart)\nsteam &lt;- read_csv(\"data-synthetic/synSteam.csv\") |&gt;\n  mutate(sessionStart = date + hours(hour))\n\n\n\n\n6 Clean diary\nHere we want to: - recode the diary data to numeric values - calculate mean scores of relevant variables - calculate within- and between-person centered variables - recode the displaced activity data into categories (randomly assigned, for now) - calculate some variables based on the telemetry\n\n\nShow the code\nsynDiaryClean &lt;- synDiary |&gt;\n  mutate(\n    across(starts_with(c(\"bpnsfs\", \"bangs\")), ~ case_when(\n      . %in% c(\"1 \\nStrongly Disagree\", \"1 - Not at all true\", \"1 - very strongly disagree\") ~ 1,\n      . %in% c(\"2\", \"2 - strongly disagree\") ~ 2,\n      . %in% c(\"3\", \"3 - disagree\") ~ 3,\n      . %in% c(\"4Neither Agree nor Disagree\", \"4\", \"4 - neither disagree nor agree\") ~ 4,\n      . %in% c(\"5\", \"5 - Completely true\", \"5 - agree\") ~ 5,\n      . %in% c(\"6\") ~ 6,\n      . %in% c(\"7 Strongly agree\") ~ 7,\n      TRUE ~ NA_integer_\n    ))\n  ) %&gt;%\n  # calculate mean scores of relevant variables (there is no missing data within waves)\n  mutate(\n    globalNS = rowMeans(select(., bpnsfs_1:bpnsfs_3), na.rm = TRUE),\n    globalNF = rowMeans(select(., bpnsfs_4:bpnsfs_6), na.rm = TRUE),\n    gameNS = rowMeans(select(., bangs_1:bangs_3), na.rm = TRUE),\n    gameNF = rowMeans(select(., bangs_4:bangs_6), na.rm = TRUE)\n  ) |&gt;\n  # Calculate within- and between-person centered variables\n  group_by(pid) %&gt;%\n  mutate(across(\n    c(globalNS, globalNF, gameNS, gameNF),\n    list(\n      cw = ~ . - mean(., na.rm = TRUE),\n      cb = ~ mean(., na.rm = TRUE)\n    )\n  )) %&gt;%\n  ungroup() %&gt;%\n  mutate(across(\n    ends_with(\"cb\"),\n    ~ . - mean(., na.rm = TRUE)\n  )) |&gt;\n  # to understand displaced activities, we will manually code the true\n  # participant activity data into categories. We pre-define 5 problematic\n  # displacement categories (work/school, social engagements, sleep, eating,\n  # fitness, caretaking) and one catch-all category (other), which may later be\n  # broken down into subcategories.\n  mutate(\n    displacedActivityCategory = ifelse(!is.na(displacedActivity),\n      sample(c(\"work/school\", \"social engagements\", \"sleep\", \"eating\", \"fitness\", \"caretaking\", \"other\"),\n        n(),\n        prob = c(.05, .05, .05, .05, .05, .05, .75),\n        replace = TRUE\n      ),\n      NA_character_\n    ),\n    displacedCoreDomain = ifelse(displacedActivityCategory %in% c(\n      \"work/school\", \"social engagements\",\n      \"sleep\", \"eating\", \"fitness\", \"caretaking\", \"other\"\n    ),\n    TRUE,\n    FALSE\n    ),\n    .after = displacedActivity\n  )\n\n\n\n\n7 Clean panel\nHere we want to: - recode the panel data to numeric values - calculate mean scores of relevant variables - calculate within- and between-person centered variables\n\n\nShow the code\nsynPanelClean &lt;- synPanel |&gt;\n  mutate(\n    across(starts_with(c(\"bangs\", \"wemwbs\", \"promis\", \"trojan\", \"BFI\")), ~ case_when(\n      . %in% c(\"Greatly interfered\") ~ -3,\n      . %in% c(\"Moderately interfered\") ~ -2,\n      . %in% c(\"Slightly interfered\") ~ -1,\n      . %in% c(\"No impact\") ~ 0,\n      . %in% c(\n        \"1 \\nStrongly Disagree\", \"1 - Not at all true\", \"1 - None of the time\", \"Never\", \"1 - Strongly disagree\",\n        \"Disagree strongly\", \"Slightly supported\"\n      ) ~ 1,\n      . %in% c(\"2\", \"2 - Rarely\", \"Rarely\", \"Disagree a little\", \"Moderately supported\") ~ 2,\n      . %in% c(\"3\", \"3 - Some of the time\", \"Sometimes\", \"Neutral; no opinion\", \"Greatly supported\") ~ 3,\n      . %in% c(\"4Neither Agree nor Disagree\", \"4\", \"4 - Often\", \"Often\", \"Agree a little\") ~ 4,\n      . %in% c(\"5\", \"5 - Completely true\", \"5 - All of the time\", \"Always\", \"5 - Strongly agree\", \"Agree strongly\") ~ 5,\n      . %in% c(\"6\") ~ 6,\n      . %in% c(\"7 Strongly agree\") ~ 7,\n      TRUE ~ NA_integer_\n    ))\n  ) %&gt;%\n  # calculate mean scores of relevant variables (there is no missing data within waves)\n  mutate(\n    wemwbs = rowMeans(select(., wemwbs_1:wemwbs_7), na.rm = TRUE),\n    promis = rowMeans(select(., promis_1:promis_8), na.rm = TRUE),\n    gameNS = rowMeans(select(., bangs_1:bangs_3, bangs_7:bangs_9, bangs_13:bangs_15), na.rm = TRUE),\n    gameNF = rowMeans(select(., bangs_4:bangs_6, bangs_10:bangs_12, bangs_16:bangs_18), na.rm = TRUE)\n  ) |&gt;\n  # Calculate within- and between-person centered variables\n  group_by(pid) %&gt;%\n  mutate(across(\n    c(wemwbs, promis, gameNS, gameNF),\n    list(\n      cw = ~ . - mean(., na.rm = TRUE),\n      cb = ~ mean(., na.rm = TRUE)\n    )\n  )) %&gt;%\n  ungroup() %&gt;%\n  mutate(across(\n    ends_with(\"cb\"),\n    ~ . - mean(., na.rm = TRUE)\n  ))\n\n\n\n\n8 Process telemetry\n\n\nShow the code\n# TODO: make sure that sessions happening as late as 6 hours later are still included, even if this isn't on the same day\n\n# in the below, we join the survey data to each telemetry table,\n# and filter for only the sessions/rows that happened immediately after the survey was completed.\n# after, we join these back together with a binary indicator of whether at least one session occurred\n\nnintendoOverlaps &lt;- synDiaryClean %&gt;%\n  left_join(nin, by = c(\"pid\", \"day\", \"date\")) |&gt;\n  filter(\n    sessionEnd &gt;= surveyCompletionTime | # Session ended after the survey time\n      sessionStart &lt;= surveyCompletionTime + days(1) # Session started before the end of the time window\n  ) |&gt;\n  group_by(pid, day, date) |&gt;\n  summarize(playedLaterNintendo = TRUE, .groups = \"drop\")\n\nxboxOverlaps &lt;- synDiaryClean %&gt;%\n  left_join(xbox, by = c(\"pid\", \"day\", \"date\")) |&gt;\n  filter(\n    sessionEnd &gt;= surveyCompletionTime | # Session ended after the survey time\n      sessionStart &lt;= surveyCompletionTime + days(1) # Session started before the end of the time window\n  ) |&gt;\n  group_by(pid, day, date) |&gt;\n  summarize(playedLaterXbox = TRUE, .groups = \"drop\")\n\nsteamOverlaps &lt;- synDiaryClean %&gt;%\n  left_join(steam, by = c(\"pid\", \"day\", \"date\")) |&gt;\n  filter(\n    sessionStart &lt;= surveyCompletionTime + days(1) # Session started before the end of the time window\n  ) |&gt;\n  group_by(pid, day, date) |&gt;\n  summarize(playedLaterSteam = TRUE, .groups = \"drop\")\n\n\n# Step 3: Determine if any Nintendo sessions occurred in the time window for each row in df\nsynDiaryClean &lt;- synDiaryClean |&gt;\n  left_join(\n    nintendoOverlaps,\n    by = c(\"pid\", \"day\", \"date\")\n  ) |&gt;\n  left_join(\n    xboxOverlaps,\n    by = c(\"pid\", \"day\", \"date\")\n  ) |&gt;\n  left_join(\n    steamOverlaps,\n    by = c(\"pid\", \"day\", \"date\")\n  ) |&gt;\n  mutate(\n    playedLaterNintendo = if_else(is.na(playedLaterNintendo), FALSE, playedLaterNintendo),\n    playedLaterXbox = if_else(is.na(playedLaterXbox), FALSE, playedLaterXbox),\n    playedLaterSteam = if_else(is.na(playedLaterSteam), FALSE, playedLaterSteam),\n    playedLaterAny = ifelse(playedLaterNintendo | playedLaterXbox | playedLaterSteam, TRUE, FALSE)\n  )\n\n\n\n\n9 Save data\n\n\nShow the code\nwrite_csv(synDiaryClean, \"data-synthetic/synDiaryClean.csv\")\nwrite_csv(synPanelClean, \"data-synthetic/synPanelClean.csv\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preprocess Data</span>"
    ]
  },
  {
    "objectID": "2_basicNeeds.html",
    "href": "2_basicNeeds.html",
    "title": "4  Study 1: Gaming and Basic Needs",
    "section": "",
    "text": "5 Study 1: Gaming and Basic Needs\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\nlibrary(pacman)\n\np_load(tidyverse, lme4, marginaleffects, glmmTMB, mice)\nShow the code\ndiary &lt;- read_csv(\"data-synthetic/synDiaryClean.csv\") # requires that the preprocessing script has been run\nintake &lt;- read_csv(\"data-synthetic/synIntake.csv\")\nnin &lt;- read_csv(\"data-synthetic/synNintendo.csv\")\nxbox &lt;- read_csv(\"data-synthetic/synXbox.csv\")\nsteam &lt;- read_csv(\"data-synthetic/synXbox.csv\")\nShow the code\ndat &lt;- diary |&gt;\n  left_join(intake |&gt; select(pid, age, gender, eduLevel, employment),\n    by = \"pid\"\n  ) |&gt;\n  mutate(pid = as.character(pid)) |&gt;\n  # this is needed otherwise nlme and marginaleffects don't play nicely\n  mutate(\n    gender = factor(gender),\n    eduLevel = factor(eduLevel),\n    employment = factor(employment),\n    day = factor(day)\n  )\nShow the code\ndiaryWide &lt;- diary |&gt;\n  select(-date) |&gt;\n  pivot_wider(\n    names_from = day,\n    values_from = -pid,\n    names_sep = \"_w\"\n  ) |&gt;\n  select(-starts_with(c(\"day\", \"missing\", \"surveyCompletion\", \"sd\", \"timeUse\", \"displacedActivity\", \"playedLater\")))\n\nquickpred(diaryWide)\n\nmice(data = diaryWide, m = 5, method = \"pmm\", maxit = 5, seed = 8675309)\nmice(data = diaryWide, m = 5, method = \"pmm\", maxit = 5, parallelseed = 8675309) # alternatively, try to parallelize",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Study 1: Gaming and Basic Needs</span>"
    ]
  },
  {
    "objectID": "2_basicNeeds.html#h1.-greater-in-game-need-satisfaction-is-associated-with-greater-global-need-satisfaction-h6-in-bang",
    "href": "2_basicNeeds.html#h1.-greater-in-game-need-satisfaction-is-associated-with-greater-global-need-satisfaction-h6-in-bang",
    "title": "4  Study 1: Gaming and Basic Needs",
    "section": "5.1 H1. Greater in-game need satisfaction is associated with greater global need satisfaction (H6 in BANG)",
    "text": "5.1 H1. Greater in-game need satisfaction is associated with greater global need satisfaction (H6 in BANG)\n\n\n\n\n\n\nNote\n\n\n\nHm. Something weird is happening here - there’s a small but consistent negative relationship between gameNS_cw and globalNS, even though these variables have nothing to do with each other. I suspect this is a data issue, but I’m not sure what’s going on.\n\n\nExperiences of gaming feed into and co-constitute experiences of life as a whole—experiences with games are one (greater or lesser) element of lives in general. Thus, H6 in BANG predicts: Greater in-game need satisfaction is associated with greater global need satisfaction.\nWe model this with a multilevel within-between linear regression whereby game-level need satisfaction (within- and between-centered; gameNS_cw and gameNS_cb) predicts deviation from a person’s typical globalNS (globalNS, with a random intercept).\n\n\n\n\n\n\nNote\n\n\n\nSomething super weird is happening here, with this strong negative relationship between gameNS and globalNS. I suspect this is a data issue, but I’m not sure what’s going on.\n\n\n\n\nShow the code\nh1mod &lt;- glmmTMB(globalNS ~ gameNS_cw + gameNS_cb + (1 + gameNS_cw | pid) + ar1(day + 0 | pid),\n  data = dat\n)\n\nplot_predictions(h1mod, condition = \"gameNS_cw\", vcov = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Study 1: Gaming and Basic Needs</span>"
    ]
  },
  {
    "objectID": "2_basicNeeds.html#h2.-situational-need-satisfaction-is-positively-associated-with-the-likelihood-of-playing-in-the-period-after-survey-completion-h2a-while-global-need-frustration-is-negatively-associated-h2b",
    "href": "2_basicNeeds.html#h2.-situational-need-satisfaction-is-positively-associated-with-the-likelihood-of-playing-in-the-period-after-survey-completion-h2a-while-global-need-frustration-is-negatively-associated-h2b",
    "title": "4  Study 1: Gaming and Basic Needs",
    "section": "5.2 H2. Situational need satisfaction is positively associated with the likelihood of playing in the period after survey completion (H2a), while global need frustration is negatively associated (H2b)",
    "text": "5.2 H2. Situational need satisfaction is positively associated with the likelihood of playing in the period after survey completion (H2a), while global need frustration is negatively associated (H2b)\nExperiences of need satisfaction during a particular gaming session lead players to update expectations for future experiences with the current game, similar games, and gaming as a whole, such that greater need satisfaction leads to higher expectations for future need satisfaction. Under BANG, need-related outcome expectations are conceptually similar to intrinsic motivation, and the behavioral product of these expectations is therefore greater behavioral engagement.\nThus, the model attempts to evaluate whether experiencing high game-level need satisfaction in one’s most recent session is linked with a higher likelihood of playing games again in the 24-hour period after the survey.\nAnother factor that might increase the likelihood that someone (re)turns to gaming is global need frustration. SDT predicts that (global) need frustration results in compensatory behavior—people attempt to replenish needs that are not being met by altering their behavior. The dense need satisfaction offered by games constitute one way for people to compensate. BANG operationalizes this compensatory play in via intrinsic motivation. Frustrated needs in one’s life in general make opportunities to fulfill those needs more salient, which—all else equal—manifests phenomenologically as an increased energy towards those activities. Given this, we predict: Global need frustration is associated with higher likelihood of playing in the 24-hour period after survey completion (H9 in BANG)\nAs they share an outcome variable, we model these together. We model these with a multilevel within-between logistic regression, where in-game need satisfaction and global need frustration (each within- and between-person centered; gameNS_cw, gameNS_cb, globalNF_cw, globalNF_cb) predict playedAfterSurvey, a binary variable indicating whether any play happened in the 24-hour period after diary survey completion.\nAs before, we include an AR(1) term to account for the fact that likelihood of play might be autocorrelated (if, e.g., people tend to get on a role and play multiple days in a row).\n\n\nShow the code\nh2mod &lt;- glmmTMB(\n  playedLaterAny ~ gameNS_cw + gameNS_cb + globalNF_cw + globalNF_cb +\n    (1 + gameNS_cw + globalNF_cw | pid) + ar1(day + 0 | pid),\n  data = dat,\n  family = binomial(link = \"logit\"),\n  dispformula = ~1,\n  ziformula = ~0\n)\n\nplot_predictions(h2mod, condition = \"globalNF_cw\", vcov = TRUE)\n\n\n\n\n\n\n\n\n\nShow the code\nplot_predictions(h2mod, condition = \"gameNS_cw\", vcov = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Study 1: Gaming and Basic Needs</span>"
    ]
  },
  {
    "objectID": "2_basicNeeds.html#h3.-when-gaming-displaces-a-core-life-domain-workschool-social-engagements-sleepeatingfitness-or-caretaking-global-need-satisfaction-will-be-lower-h5-in-bang",
    "href": "2_basicNeeds.html#h3.-when-gaming-displaces-a-core-life-domain-workschool-social-engagements-sleepeatingfitness-or-caretaking-global-need-satisfaction-will-be-lower-h5-in-bang",
    "title": "4  Study 1: Gaming and Basic Needs",
    "section": "5.3 H3. When gaming displaces a core life domain (work/school, social engagements, sleep/eating/fitness, or caretaking), global need satisfaction will be lower (H5 in BANG)",
    "text": "5.3 H3. When gaming displaces a core life domain (work/school, social engagements, sleep/eating/fitness, or caretaking), global need satisfaction will be lower (H5 in BANG)\nWe don’t have temporal precedence here (the need satisfaction measure refers to the day as a whole), and have very little ability to define and adjust for confounds, so this is a very weak test of the displacement hypothesis—but the first of its kind, as far as I know.\nBriefly, I’m just interested in whether gaming sessions that displace a core life domain—work/school, social engagements, sleep/eating/fitness, or caretaking—are associated with lower global need satisfaction. displacedCoreActivity is a binary variable; participants write in a free text response what they most likely would have done instead of their most recent gaming session, and these are classified into core/noncore domains.\nWe use a multilevel linear regression to determine whether displacing a core activity is likely to co-occur with a person differing from their typical level of global need satisfaction (globalNS).\n\n\nShow the code\ntable(dat$displacedCoreDomain)\n\n\n\nFALSE  TRUE \n14031  6969 \n\n\nShow the code\nh3mod &lt;- glmmTMB(globalNS ~ displacedCoreDomain + (1 + displacedCoreDomain | pid) + ar1(day + 0 | pid),\n  data = dat\n)\nplot_predictions(h3mod, condition = \"displacedCoreDomain\", vcov = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Study 1: Gaming and Basic Needs</span>"
    ]
  }
]