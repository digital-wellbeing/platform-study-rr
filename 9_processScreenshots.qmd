---
title: "Process iOS Screenshots"
output: html_document
editor_options: 
  chunk_output_type: console
execute: 
  warning: false
page-layout: full
format:
  html:
    code-fold: true
    code-summary: "Show the code"
bibliography: references.bib
---

# TODOs
- add response ID
- extract dates
- stress-test
- flag invalid screenshots

# DONE
- differentiate light/dark mode
- refactor

# Load packages

```{r}
#| label: setup

if (!require("pacman")) install.packages("pacman");
library(pacman)
p_load(tidyverse, dplyr, ggplot2, forcats, tesseract, magick, colorfindr, janitor, tictoc)

knitr::opts_chunk$set(
  echo = knitr::is_html_output(),
  warning = FALSE,
  message = FALSE,
  output = TRUE
)

options(scipen = 999)

eng <- tesseract("eng")
```

```{r}
#| label: set-folder

screenshots <- list.files(path = Sys.getenv("screenshotsPath"), 
                          full.names = TRUE)
```

# Define functions for later use

## Function: Process images

```{r}
#| label: process-images

processImage <- function(file) {

  info <- image_read(file) |> 
    image_info()
  
  # set thresholds for light mode vs dark mode screenshots
  if ("#FFFFFF" %in% get_colors(file, min_share = 0.30)$col_hex) {
    whiteThreshold <- "90%"
    blackThreshold <- "80%"
  } else {
    whiteThreshold <- "30%"
    blackThreshold <- "95%"
  }
  
  # load and de-noise the screenshot 
  image_read(file, density = 300) |> 
    
    # thresholding increases the contrast in letters - this will need to be adjusted for light vs. dark mode screenshots
    image_threshold(type = "white",
                    threshold = whiteThreshold,
                    channel = NULL) |>
    image_threshold(type = "black",
                    threshold = blackThreshold,
                    channel = NULL) |>
    image_deskew(threshold = 40) |> 
    image_contrast(sharpen = 1) |> 
    image_crop(paste0(info$width*.91, "x", info$height, "-80"))
}

convertMinutes <- function(time_str) {
  hours <- as.numeric(str_extract(time_str, "\\d+(?=h)"))
  minutes <- as.numeric(str_extract(time_str, "\\d+(?=m)"))
  hours[is.na(hours)] <- 0  # Replace NA hours with 0
  minutes[is.na(minutes)] <- 0  # Replace NA minutes with 0
  total_minutes <- hours * 60 + minutes
  return(total_minutes)
}
```

## Function: Process text

```{r}
#| label: process-text

# extract and organize relevant data
processText <- function(text) {
  
  text |> 
    str_remove_all("(?s).*APPS & WEBS\\s*\\n") |> 
    str_remove_all("(?i)Show More.*") |> 
    str_split_1("\n") |> 
    as_tibble_col() |> 
    filter(
      value %in% categories |  # Words (including &, spaces, and \)
        str_detect(value, "^\\d+h\\s+\\d+m$") |     # Time format Xh Ym
        str_detect(value, "^(Th|th)\\s+\\d+m$") |         # Time format Th Ym
        str_detect(value, "^\\d+m$") # Time format Ym
    ) |> 
    filter(lag(value) %in% categories | value %in% categories) |> # only keep the timestamps that match up with the categories
    mutate(
      value = str_replace(value, "^(Th|th)", "1h")     # Replace Th with 1h
    ) |> 
    
    # convert to 2 columns, one for category and 1 for time
    mutate(row_num = (row_number() + 1) %/% 2) |> 
    group_by(row_num) |> 
    summarise(
      category = first(value),
      time = last(value),
      .groups = "drop"
    ) |> 
    select(-row_num) |> 
    mutate(minutes = convertMinutes(time))
}
```

## Function: Master pipeline - processImag + OCR + processText

```{r}
#| label: master-function

# Define a function to process each file
extractData <- function(file) {
  
  file |> 
    processImage() |> 
    ocr(engine = tesseract(options = list(tessedit_pageseg_mode = 11))) |> 
    processText()

}
```


## Flag for manual extraction

*TODO*: flag images that are unlikely to be extractable using OCR and may need subsequent manual extraction or may not have valid data at all. Common indicators of this are other languages or the wrong tab selected in the screenshot.

```{r}
#| label: flag-manual-extraction

pre_identification_manual <- function(text) {
  flag_wrong_screenshot_1 <- grepl("SHOW APPS & WEBSITES", text, fixed = TRUE) #reason 1
  flag_wrong_screenshot_2 <- grepl("Samsung S6 Note Tablet", text, fixed = TRUE) #reason 2
  flag_wrong_screenshot_3 <- grepl("Most used apps", text, fixed = TRUE) #reason 3
  flag_wrong_screenshot_4 <- grepl("Settimana scorsa", text, fixed = TRUE) #spanish 1
  flag_wrong_screenshot_5 <- grepl("UTILIZZATE MOSTRA ", text, fixed = TRUE) #spanish 2
  
  if (flag_wrong_screenshot_1 | flag_wrong_screenshot_2 | flag_wrong_screenshot_3 | flag_wrong_screenshot_4){
    return(TRUE)
  }
  return(FALSE)
}
```

## Debug

```{r}
#| label: debug
#| eval: false

screenshots[6] |>
  processImage() |>
  ocr(engine = tesseract(options = list(tessedit_pageseg_mode = 11))) |>
  processText()

```


# Extract Data from Images

Now to the meat of the endeavor - for each file in the screenshots data, we will:
1. Pre-process the image
2. Perform OCR
3. Extract and organize relevant data

```{r}
#| label: extract-data

# Define categories to look for in the text - to be updated and expanded in light of new categories, OCR errors, and so on
categories <- c("Entertainment", "Social", "Information & Reading", "Games", "Productivity & Finance", "Travel", "Other")

# Perform OCR
tic() # set timer
results <- map_df(screenshots, extractData) |> 
  rbind()
toc()
```

# Save Data:

```{r}
#| label: save-data

write_csv("data/results.csv")
```
